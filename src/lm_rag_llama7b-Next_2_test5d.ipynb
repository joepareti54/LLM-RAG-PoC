{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf15cd1-e826-44cd-8b1d-bb8ee86ea7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch>=2.0.0 transformers>=4.31.0 sentence-transformers faiss-cpu PyMuPDF numpy accelerate 'bitsandbytes>=0.41.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "484b0ad2-7d52-4061-9cf6-fb329c37b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import fitz\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6134657-bca2-4222-8e05-abb79bed6035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACE_TOKEN'] = 'your token here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9b04fa2-8a9e-455d-9511-ef5b401a3eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    def __init__(self, directory_path):\n",
    "        self.directory_path = directory_path\n",
    "        self.documents = []\n",
    "        self.doc_titles = []\n",
    "        self.embed_model = None\n",
    "        self.index = None\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        # auto-detect GPU or CPU\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def load_pdfs(self):\n",
    "        pdf_files = [f for f in os.listdir(self.directory_path) if f.endswith('.pdf')]\n",
    "        for f in pdf_files:\n",
    "            full_path = os.path.join(self.directory_path, f)\n",
    "            text = self.extract_text_from_pdf(full_path)\n",
    "            if text:\n",
    "                self.documents.append(text)\n",
    "                self.doc_titles.append(f)\n",
    "        print(f\"Loaded {len(self.documents)} PDFs.\")\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        doc_text = []\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            for page in doc:\n",
    "                txt = page.get_text()\n",
    "                if txt.strip():\n",
    "                    doc_text.append(txt)\n",
    "            doc.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {pdf_path} - {e}\")\n",
    "            return None\n",
    "        return \" \".join(doc_text)\n",
    "\n",
    "    def init_embeddings(self):\n",
    "        self.embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"SentenceTransformer loaded.\")\n",
    "\n",
    "    def create_index(self):\n",
    "        print(\"Creating FAISS index.\")\n",
    "        embeddings = self.embed_model.encode(self.documents, show_progress_bar=True)\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.index.add(np.array(embeddings).astype('float32'))\n",
    "        print(\"Index created and docs embedded.\")\n",
    "\n",
    "    def init_llama(self):\n",
    "        print(\"Loading Llama 2 7B in 4-bit.\")\n",
    "        model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "        self.model = LlamaForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            # Optional: set your cache_dir if needed\n",
    "            cache_dir='/media/joepareti54/Elements/x/huggingface/hub/more/',\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"Llama 7B model loaded in 4-bit.\")\n",
    "\n",
    "    def retrieve_documents(self, query, k=3):\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        query_embed = self.embed_model.encode([query])[0]\n",
    "        distances, indices = self.index.search(np.array([query_embed]).astype('float32'), k)\n",
    "        retrieved_docs = []\n",
    "        for rank, idx in enumerate(indices[0]):\n",
    "            dist = distances[0][rank]\n",
    "            doc_text = self.documents[idx]\n",
    "            doc_title = self.doc_titles[idx]\n",
    "            print(f\"  Rank {rank+1}, Distance={dist:.4f}, Doc={doc_title}\")\n",
    "            retrieved_docs.append(doc_text)\n",
    "        return retrieved_docs\n",
    "\n",
    "    def summarize_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Step 1: Summarize the doc text in a short prompt so we don't pass huge data to final generation.\n",
    "        \"\"\"\n",
    "        # Possibly truncate text first if it's huge\n",
    "        max_chars = 1000\n",
    "        truncated_text = text[:max_chars]\n",
    "\n",
    "        prompt = (\n",
    "            \"You are an AI summarizer. Summarize the following text in your own words. \"\n",
    "            \"Do NOT repeat large chunks verbatim.\\n\\n\"\n",
    "            f\"{truncated_text}\\n\\n\"\n",
    "            \"Summary:\"\n",
    "        )\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(self.device)\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=100,\n",
    "            num_beams=1,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        return summary\n",
    "\n",
    "    def generate_final_answer(self, summary: str, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Step 2: Use the doc summary plus user query to produce a final answer.\n",
    "        \"\"\"\n",
    "        prompt = (\n",
    "            \"You are a knowledgeable AI with broad expertise.\\n\"\n",
    "            f\"Here is a summary of a reference document:\\n{summary}\\n\\n\"\n",
    "            f\"User question: {query}\\n\\n\"\n",
    "            \"Answer in your own words using the summary. Do not repeat the summary verbatim:\"\n",
    "        )\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(self.device)\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=150,\n",
    "            num_beams=1,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        return answer\n",
    "\n",
    "    def run_query(self, query, k=3):\n",
    "        docs = self.retrieve_documents(query, k)\n",
    "        if not docs:\n",
    "            print(\"No docs returned. Possibly the index is empty.\")\n",
    "            return\n",
    "\n",
    "        # Optionally combine or just pick the top doc\n",
    "        # e.g., summarizing only the 1st doc for demonstration\n",
    "        # or you can merge them if you want multiple summaries\n",
    "        doc_text = docs[0]\n",
    "\n",
    "        # Step 1: Summarize doc text\n",
    "        doc_summary = self.summarize_text(doc_text)\n",
    "\n",
    "        # Step 2: Generate final answer\n",
    "        final_answer = self.generate_final_answer(doc_summary, query)\n",
    "        print(f\"\\nAnswer:\\n{final_answer}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c245caae-9e47-4874-9d2d-1b5a0faacb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2665 PDFs.\n",
      "SentenceTransformer loaded.\n",
      "Creating FAISS index.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1dd9550cb14288b685612a618e3a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created and docs embedded.\n",
      "Loading Llama 2 7B in 4-bit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f71493886fc4aebaa65a6703db20003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 7B model loaded in 4-bit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter query (or 'quit'):  is 5g an important techology for car manufacturing ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: is 5g an important techology for car manufacturing ?\n",
      "  Rank 1, Distance=0.8409, Doc=5G BlogFinal-0x.pdf\n",
      "  Rank 2, Distance=0.9388, Doc=Everything You Need to Know About 5G - WSJ.pdf\n",
      "  Rank 3, Distance=0.9529, Doc=WP_5G_for_Connected_Industries_and_Automation_Download_19.03.19.pdf\n",
      "\n",
      "Answer:\n",
      "You are a knowledgeable AI with broad expertise.\n",
      "Here is a summary of a reference document:\n",
      "You are an AI summarizer. Summarize the following text in your own words. Do NOT repeat large chunks verbatim.\n",
      "\n",
      " \n",
      "1 \n",
      " \n",
      "5G Technologies in the Manufacturing Industry: Opportunities and \n",
      "Challenges \n",
      "Wednesday, February 19, 2020  joepareti54@mail.com \n",
      "Contents \n",
      "5G Technologies in the Manufacturing Industry: Opportunities and Challenges ........................................ 1 \n",
      "Key Points .................................................................................................................................................. 1 \n",
      "The role of Software and Standardization to reap the 5G benefits.......................................................... 1 \n",
      "Standards .............................................................................................................................................. 2 \n",
      "Challenges ............................................................................................................................................. 2 \n",
      "German Automotive OEMs roll out 5G technology in industrial campus networks ................................ 3 \n",
      "Investments outlook ac\n",
      "\n",
      "Summary:\n",
      "The article discusses the opportunities and challenges of 5G technologies in the manufacturing industry. It highlights the importance of software and standardization to reap the benefits of 5G. The article also mentions the roll-out of 5G technology in industrial campus networks by German Automotive OEMs. The investment outlook for 5G in the manufacturing industry is also discussed.\n",
      "\n",
      "In your own words, what is the main idea of\n",
      "\n",
      "User question: is 5g an important techology for car manufacturing ?\n",
      "\n",
      "Answer in your own words using the summary. Do not repeat the summary verbatim:\n",
      "\n",
      "5G technology has the potential to transform the manufacturing industry by providing faster data speeds, lower latency, and increased connectivity. However, there are challenges to be addressed, such as standardization and software development, in order to fully reap the benefits of 5G. German Automotive OEMs are already investing in 5G technology for use in industrial campus networks. The investment outlook for 5G in the manufacturing industry is positive, with the potential for increased efficiency, productivity, and innovation.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter query (or 'quit'):  what are important technologies to accelerate product development\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: what are important technologies to accelerate product development\n",
      "  Rank 1, Distance=1.0854, Doc=‘Predictive-Maintenance’ Tech Is Taking Off as Manufacturers Seek More Efficiency - WSJ.pdf\n",
      "  Rank 2, Distance=1.1008, Doc=Mayo Clinic, Others Use ‘AI Factories’ to Speed AI Development.pdf\n",
      "  Rank 3, Distance=1.1131, Doc=New Research Busts Popular Myths About Innovation - WSJ.pdf\n",
      "\n",
      "Answer:\n",
      "You are a knowledgeable AI with broad expertise.\n",
      "Here is a summary of a reference document:\n",
      "You are an AI summarizer. Summarize the following text in your own words. Do NOT repeat large chunks verbatim.\n",
      "\n",
      "Startups that make technology designed to predict industrial equipment failures before\n",
      "they happen are seeing a surge in demand, as strained supply chains prompt\n",
      "manufacturers to squeeze more efficiency out of production lines, startup founders and\n",
      "analysts say.\n",
      "Anna Farberov,\n",
      "general manager of\n",
      "PepsiCo\n",
      "Labs, the technology venture arm of PepsiCo\n",
      "Inc., said that over the past year so-called predictive-maintenance systems at four Frito-\n",
      "Lay plants reduced unexpected breakdowns, interruptions and incremental costs for\n",
      "replacement parts, among other benefits.\n",
      "Developed by New York-based startup Augury Inc., the technology has helped Frito-Lay\n",
      "add some 4,000 hours a year of manufacturing capacity—the equivalent of several million\n",
      "pounds of snacks coming off the production line and shipped to store shelves, Ms.\n",
      "Farberov said.\n",
      "PepsiCo is now sending the technology to most\n",
      "of its U.S. Frito-Lay plants, and plans to roll it\n",
      "out in its Southern U.S. beverage plants and eventually to all its bot\n",
      "\n",
      "Summary:\n",
      "Startups that make technology to predict industrial equipment failures are seeing increased demand as manufacturers look for ways to improve efficiency and reduce costs. PepsiCo has seen success with this technology at four of its Frito-Lay plants, adding over 4,000 hours of manufacturing capacity and reducing unexpected breakdowns, interruptions, and replacement part costs. The technology, developed by Augury Inc., is now being rolled out to more of PepsiCo'\n",
      "\n",
      "User question: what are important technologies to accelerate product development\n",
      "\n",
      "Answer in your own words using the summary. Do not repeat the summary verbatim:\n",
      "\n",
      "The summary highlights the growing demand for predictive maintenance technologies in the industrial sector, as manufacturers look to optimize their production lines and reduce costs. PepsiCo has seen significant benefits from implementing this technology at four of its Frito-Lay plants, including increased manufacturing capacity and reduced unexpected breakdowns and replacement part costs. As a result, the company is now rolling out this technology to more of its plants, including its Southern U.S. beverage plants. Other important technologies to accelerate product development in this space include machine learning and IoT capabilities, which can help improve predictive accuracy and streamline the maintenance process.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter query (or 'quit'):  are humans an evolution step of apes or of fish\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: are humans an evolution step of apes or of fish\n",
      "  Rank 1, Distance=1.4519, Doc=Startup Fauna Bio Studies Animal Genomes for Clues to Human Diseases - WSJ.pdf\n",
      "  Rank 2, Distance=1.5220, Doc=Learning from evolution_ Using AI language models to design proteins.pdf\n",
      "  Rank 3, Distance=1.5617, Doc=Google Debuts Cybersecurity-Focused AI System.pdf\n",
      "\n",
      "Answer:\n",
      "You are a knowledgeable AI with broad expertise.\n",
      "Here is a summary of a reference document:\n",
      "You are an AI summarizer. Summarize the following text in your own words. Do NOT repeat large chunks verbatim.\n",
      "\n",
      "This copy is for your personal, non-commercial use only. Distribution and use of this material are governed by our Subscriber Agreement and by copyright law. For\n",
      "non-personal use or to order multiple copies, please contact Dow Jones Reprints at 1-800-843-0008 or visit www.djreprints.com.\n",
      "https://www.wsj.com/articles/startup-fauna-bio-studies-animal-genomes-for-clues-to-human-diseases-cf5f86dc\n",
      "INDUSTRY NEWS\n",
      "Startup Fauna Bio Studies Animal Genomes for Clues to Human Diseases\n",
      "Fauna Bio compares genomic data from humans and other species to find drug targets. Its obesity research with collaborator Eli Lilly involves the 13-lined ground squirrel.\n",
      "By Brian Gormley\n",
      "Dec. 21, 2023 6:00 am ET | WSJ PRO\n",
      "Fauna Bio co-founders, from left to right, Linda Goodman, Ashley Zehnder and Katie Grabek. PHOTO: FAUNA BIO\n",
      "Biotechnology startup Fauna Bio and drugmaker Eli Lilly are collaborating to discover drug targets for obesity through a process that involves studying the genetics of animals that survive \n",
      "\n",
      "Summary:\n",
      "Fauna Bio is a biotech startup that is using comparative genomics to find drug targets for human diseases. The company is working with Eli Lilly to study the genomes of animals, such as the 13-lined ground squirrel, to identify potential drug targets for obesity. The approach involves comparing the genomes of humans and other species to find genetic variations that could be targeted with drugs.\n",
      "\n",
      "User question: are humans an evolution step of apes or of fish\n",
      "\n",
      "Answer in your own words using the summary. Do not repeat the summary verbatim:\n",
      "Humans are not an evolutionary step of apes or fish. Rather, humans and other primates evolved from a common ancestor. The study of comparative genomics, such as that conducted by Fauna Bio, can help scientists identify genetic variations that may have contributed to the evolution of humans and other primates.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    directory_path = \"/media/joepareti54/Elements/x/All_Finance_PDF_files_old/\"\n",
    "# update the line above using your own storage\n",
    "    rag = SimpleRAG(directory_path)\n",
    "    rag.load_pdfs()\n",
    "    rag.init_embeddings()\n",
    "    rag.create_index()\n",
    "    rag.init_llama()\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\nEnter query (or 'quit'): \").strip()\n",
    "        if query.lower() == 'quit':\n",
    "            break\n",
    "        rag.run_query(query, k=3)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
